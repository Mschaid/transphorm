{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import scipy.io\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from typing import Literal\n",
    "import torch \n",
    "from torch import nn\n",
    "from transphorm.model_components import AATrialDataModule, TrialClassifer, LSTMClassifer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "data_path = Path(os.getenv('TRIAL_DATA_PATH'))\n",
    "model_path = \"/Users/mds8301/Development/transphorm/models/aa-classifiers/a0383409f8fe4c3683f83cda885528e0/checkpoints/trial_lstm_clf_v2.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 6104\n",
    "hidden_size = 8\n",
    "num_lay = 2\n",
    "\n",
    "class LSTMClassifer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout1d(.2)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # init hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros_like(h0).to(x.device)\n",
    "\n",
    "        x_1, (h0, c0) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # batch_norm = self.batch_norm(h0[-1])\n",
    "        # dropout = self.dropout(batch_norm)\n",
    "        # out = self.fc(dropout)\n",
    "\n",
    "        return x_1, (h0, c0)\n",
    "model = LSTMClassifer(input_size, hidden_size, num_lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod = AATrialDataModule(data_path)\n",
    "data_mod.prepare_data()\n",
    "data_mod.setup('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_mod.train[0:32][0]\n",
    "x_1,(h0, c0) = model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0[-1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transphorm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
