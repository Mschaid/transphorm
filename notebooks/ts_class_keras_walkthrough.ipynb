{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN, MiniBatchKMeans, KMeans \n",
    "from transphorm.preprocessors import save_array_as_df_parquet\n",
    "import scipy.fft as spft\n",
    "import seaborn as sns\n",
    "from transphorm.pipelines import reshape_for_multivariable, downsample\n",
    "from sktime.clustering.k_means import TimeSeriesKMeans \n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_path = Path(\"/Volumes/fsmresfiles/Basic_Sciences/Phys/Lerner_Lab_tnl2633/Gaby/Data Analysis/ActiveAvoidance/Core_guppy_postcross/core_data\")\n",
    "ts_path = main_path / \"dopamine_full_timeseries_array.npy\"\n",
    "per_av_path = main_path/\"percent_avoid.parquet\"\n",
    "\n",
    "data = torch.Tensor(np.load(ts_path))\n",
    "data = data[~torch.isnan(data[:,0])]\n",
    "# data = reshape_for_multivariable(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:,0]\n",
    "X = data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "# resahpe for multivariate with 1 channel- this allows for making any a model that is applicable to multivariate_\n",
    "# def process_arr(arr):\n",
    "#     return (downsample(\n",
    "#             reshape_for_multivariable(arr)\n",
    "#         ))\n",
    "\n",
    "X_train, X_test = reshape_for_multivariable(X_train), reshape_for_multivariable(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2768],\n",
       "         [ 0.2723],\n",
       "         [ 0.2679],\n",
       "         ...,\n",
       "         [-1.1605],\n",
       "         [-1.1596],\n",
       "         [-1.1587]],\n",
       "\n",
       "        [[ 0.2170],\n",
       "         [ 0.2282],\n",
       "         [ 0.2395],\n",
       "         ...,\n",
       "         [ 1.3609],\n",
       "         [ 1.3590],\n",
       "         [ 1.3570]],\n",
       "\n",
       "        [[ 2.6965],\n",
       "         [ 2.6985],\n",
       "         [ 2.7005],\n",
       "         ...,\n",
       "         [ 0.6903],\n",
       "         [ 0.6837],\n",
       "         [ 0.6770]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5125],\n",
       "         [-0.5147],\n",
       "         [-0.5169],\n",
       "         ...,\n",
       "         [-0.3195],\n",
       "         [-0.3117],\n",
       "         [-0.3038]],\n",
       "\n",
       "        [[-0.3850],\n",
       "         [-0.3753],\n",
       "         [-0.3656],\n",
       "         ...,\n",
       "         [-0.6457],\n",
       "         [-0.6432],\n",
       "         [-0.6408]],\n",
       "\n",
       "        [[-0.0443],\n",
       "         [-0.0306],\n",
       "         [-0.0170],\n",
       "         ...,\n",
       "         [ 0.3108],\n",
       "         [ 0.3348],\n",
       "         [ 0.3590]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique classes\n",
    "classes = np.unique(y_train)\n",
    "numb_classes = classes.shape[0]\n",
    "numb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_classier = KNeighborsTimeSeriesClassifier(algorithm='brute_incr')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier= KNeighborsTimeSeriesClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "min_threshold = np.min(np.max(pca, axis = 0))\n",
    "min_threshold\n",
    "pca_no_outliers = np.where(pca < min_threshold, pca, np.nan)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection = '3d')\n",
    "pca_df = pl.DataFrame(pca_no_outliers, schema = ['p1', 'p2', 'p3']).with_columns(pl.lit(y).alias('learned'))\n",
    "ax.scatter(pca_no_outliers[:,0], pca_no_outliers[:,2], pca_no_outliers[:,1], c = y)\n",
    "# sns.scatterplot(pca_df, x = 'p2', y = 'p3', hue = 'learned', ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_.shape\n",
    "pca_df = pl.DataFrame(pca, schema = [f'pc_{i}' for i in range(pca.shape[1])]).with_columns(pl.lit(means.labels_).alias('clusters'), )\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 1.0173e3\n",
    "sec_1 = sample_rate\n",
    "ms_200 = sec_1/5\n",
    "ms_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_class_1 = X_train[y_train==classes[0]][0]\n",
    "example_class_2 = X_train[y_train==classes[1]][0]\n",
    "example_class_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(example_class_1)\n",
    "plt.plot(example_class_2)\n",
    "plt.xscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data to use validation_split in training\n",
    "idx = np.random.permutation(len(X_train))\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, headsize, num_heads, ff_dim, dropout = 0):\n",
    "    # attention and normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim = headsize,\n",
    "        num_heads= num_heads, \n",
    "        dropout=dropout)(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x+inputs\n",
    "\n",
    "    #feedforward\n",
    "    x = layers.Conv1D(filters = ff_dim, kernel_size=1, activation='relu')(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters = inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "# build model\n",
    "def build_model(\n",
    "    input_shape, \n",
    "    head_size, \n",
    "    num_heads, \n",
    "    ff_dim,\n",
    "    num_transformer_blocks, \n",
    "    mlp_units, \n",
    "    dropout = 0, \n",
    "    mlp_dropout = 0):\n",
    "    \n",
    "    inputs = keras.Input(shape = input_shape)\n",
    "    x = inputs\n",
    "    for x in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout = dropout)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D(data_format = 'channels_last', dropout = dropout)(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation='relu')(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(numb_classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "    \n",
    "    \n",
    "# train and eval\n",
    "input_shape = X_train.shape[1:]\n",
    "model = build_model(input_shape = input_shape, head_size=256, num_heads=4, ff_dim = 4, num_transformer_blocks=4, mlp_units = [128], mlp_dropout = 0.4, dropout=0.25)\n",
    "model.compile(\n",
    "    loss = \"sparse_categorical_crossentropy\", \n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 1e-4),\n",
    "    metrics = ['sparse_categorical_accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=5, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(2, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=X_train.shape[1:])\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 32\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\", save_best_only = True, monitor = \"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor = \"val_loss\", factor = 0.5, patience=0.5, min_lr = 0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 50, verbose = 1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer = \"adam\", \n",
    "    loss = \"sparse_categorical_crossentropy\", \n",
    "    metrics = [\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size= batch_size, \n",
    "    epochs=epochs, \n",
    "    # callbacks=callbacks,\n",
    "    validation_split=0.2, \n",
    "    verbose = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transphorm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
